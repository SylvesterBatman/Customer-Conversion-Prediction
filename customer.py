# -*- coding: utf-8 -*-
"""Customer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ZlsDAic6NYrBKwXVOx2VOFG6gi7H_Gj

#Intro
"""

import numpy as np
import pandas as pd

from matplotlib import pyplot as plt

import seaborn as sns

fp = pd.read_csv('C:\\Users\\Sylvester\\Desktop\\Streamlit\\Practice\\train.csv')
fp

target_count = fp.y.value_counts()
print('class 0:', target_count[0])
print('class 1:', target_count[1])
print('Proportion of class 0 is ',round(target_count[0] * 100 / (target_count[1] + target_count[0]), 2), '%')

target_count.plot(kind='bar', title='Count(y)');

#Cleaning

fp.describe()

fp.nunique(axis=0)

fp.isnull().sum()

fp.shape

fp[fp.duplicated()]

fp=fp.drop_duplicates()

fp.shape

fp.describe()

fp.dtypes

impute = ['job','education_qual']
for column in impute:
  mode = fp[column].mode()[0]
  fp[column] = fp[column].replace("unknown",mode)

fp

#EDA

num_var = fp.select_dtypes(include=[np.number])

plt.style.use("ggplot")
for column in num_var.columns:
  plt.figure(figsize = (20,4))
  ax = plt.subplot(121)
  sns.histplot(data = fp, x = column, kde = True, bins = 50)
  plt.title(column)

plt.style.use("ggplot")
for column in num_var.columns:
  plt.figure(figsize = (20,4))
  ax = plt.subplot(121)
  sns.boxplot(data = fp, x = column)
  plt.title(column)

#z score
from scipy import stats
z = np.abs(stats.zscore(fp['age']))
outlier = np.array(np.where(z > 3))
print(outlier.size)

lower = []
upper = []
for i in num_var.columns:
  IQR = fp[i].quantile(0.75) - fp[i].quantile(0.25)
  lower_bound = fp[i].quantile(0.25) - (1.5*IQR)
  upper_bound = fp[i].quantile(0.75) + (1.5*IQR)

  print(i, ":", lower_bound, ",", upper_bound)

  lower.append(lower_bound)
  upper.append(upper_bound)

j = 0
for i in num_var.columns:
  fp.loc[fp[i] > upper[j], i] = int(upper[j])
  j = j+1

plt.style.use("ggplot")
for column in num_var.columns:
  plt.figure(figsize=(20,4))
  ax = plt.subplot(121)
  sns.boxplot(data = fp, x = column)
  plt.title(column)

fp.replace({'y': {'yes': 1, 'no': 0}}, inplace = True)

corr = fp.corr()
sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, annot = True, cmap = sns.diverging_palette(220, 20, as_cmap = True))

fp.groupby('job')['y'].mean().sort_values(ascending = False).plot(kind = 'bar')

cat_var = fp.select_dtypes(include = ["object"]).columns
for column in cat_var:
  fp.groupby(column)['y'].mean().sort_values(ascending = False).plot(kind = 'bar')
  plt.show()

cat_var = fp.select_dtypes(include = ["object"]).columns
for column in cat_var:
  plt.figure(figsize = (20,4))
  ax = plt.subplot(121)
  sns.countplot(data = fp, x = column, hue = 'y')
  plt.xlabel(column)
  plt.ylabel("count of customers")
  plt.title(column)

  total = sum([p.get_height() for p in ax.patches])
  for p in ax.patches:
    width = p.get_width()
    height = p.get_height()
    x, y = p.get_xy()
    ax.annotate(f'{height/total:.1%}', (x + width/2, y +height*1.02), ha = 'center')
  plt.show()



#Encode

for i in cat_var:
  print(i, ':', fp[i].unique())


fp

fp.describe()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

fpT = fp
fpT["job"] = fpT["job"].map({"technician" : 0,
                             "entrepreneur" : 1, "blue-collar" :2,
                             "management" : 3, "retired" : 4, 
                             "admin." : 5, "services" : 6, "self-employed" : 7,
                             "unemployed" : 8, "housemaid" :9, 
                             "student" : 10})

fpT["marital"] = fpT["marital"].map({"married" : 2, "single" : 1, "divorced" : 0})

fpT["education_qual"] = fpT["education_qual"].map({"tertiary" : 2, "secondary" : 1, 
                                                  "primary" : 0})

fpT["call_type"] = fpT["call_type"].map({"unknown" : 0, "cellular" : 1, 
                                         "telephone" : 2})

fpT["mon"] = fpT["mon"].map({"may" : 4, "jun" : 5, "jul" : 6, "aug" : 7, 
                             "oct" : 9, "nov" : 10, "dec" : 11, "jan" : 0, 
                             "feb" : 1,"mar" : 2, "apr" : 3, "sep" : 8})

fpT["prev_outcome"] = fpT["prev_outcome"].map({"unknown" : 0, "failure" : 2, 
                                               "other" : 3, "success" : 1})

fpT

fpT.dtypes

labels = fpT.columns[:9]

labels

X = fpT[labels]
y = fpT['y']

#Splitting



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

import imblearn


#Scaling

from sklearn.preprocessing import StandardScaler

std = StandardScaler()
x_train = std.fit_transform(X_train)
x_test = std.transform(X_test)

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

model = DecisionTreeClassifier()
model.fit(X_train[['call_type']], y_train)
y_pred = model.predict(X_test[['call_type']])

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#Confusion Matrix

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

conf_mat = confusion_matrix(y_true = y_test, y_pred = y_pred)
print('Confusion matrix:\n', conf_mat)

labels = ['Class 0', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, model.predict_proba(X_test[['call_type']])[:, 1])

import imblearn

#Over-sampling followed by Under-sampling



from imblearn.combine import SMOTEENN

smt = SMOTEENN(sampling_strategy = 'all')
X_smt, y_smt = smt.fit_resample(X, y)

X_smt.shape, y_smt.shape

#Max Voting

from sklearn.ensemble import VotingClassifier # this is the function that ensembles my model
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score

model1 = LogisticRegression(random_state=1)
model2 = tree.DecisionTreeClassifier(random_state=1)
model3 = KNeighborsClassifier(3)
model = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('knn',model3)], voting='soft') # it will stitich all the models together, voting = hard means max voting 
# we need to give the list of models that we are trying to combine
model.fit(X_smt, y_smt) # to train all of the models
preds = model.predict(X_test)
#model.score(x_test,y_test) # prediction
roc_auc_score(y_test,model.predict_proba(X_test)[:,1])

model3 = KNeighborsClassifier(3)
model3.fit(X_smt, y_smt)
roc_auc_score(y_test,model3.predict_proba(X_test)[:,1])

#XGBoost

import warnings
warnings.filterwarnings('ignore')

import xgboost as xgb
from sklearn.model_selection import cross_val_score
import numpy as np
for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:
  model = xgb.XGBClassifier(learning_rate = lr, n_estimators=100, verbosity = 0) # initialise the model
  model.fit(X_smt,y_smt) #train the model
  roc_auc_score(y_test,model.predict_proba(X_test)[:,1])
  print("Learning rate : ", lr, " Train score : ", model.score(X_smt,y_smt), " Cross-Val score : ", np.mean(cross_val_score(model, X_smt, y_smt, cv=10)))

model = xgb.XGBClassifier(learning_rate = 1, n_estimators = 100)





model.fit(X_smt, y_smt)
roc_auc_score(y_test,model.predict_proba(X_test)[:,1])

